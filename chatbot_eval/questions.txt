
# Experimenting with different instructions
Q: Did you experiment with different instructions? If so, what variations did you try and what were the results?

I experimented with several variations of instructions to optimize the model's performance. Below are the key iterations and findings:

- Initial Prompt Design and Considerations -
1. Hashtag Formatting:  
   - Switched from space-separated to comma-separated hashtags to clarify that hashtags are distinct entities.  
   - Ensured the classification format matched the demonstration examples for consistency and easier learning.

2. Response Processing:
   - I initially struggled with handling model responses containing irrelevant text alongside hashtags. Solved this by using regex to extract only valid hashtags from the response.

- Prompt Variations and Results -
Prompt 1: "Below are customer reviews and the hashtags that describe them:\n\n{demonstrations_text}\nNow classify the following review:\nReview: {review}\nHashtags:"
- Changes:  
  - Used "hashtags" terminology instead of "categories" for consistency with the demonstration examples and first sentence of the prompt.
- Finding: This prompt worked reasonably well but left room for improvement in reducing irrelevant output.

Prompt 2: "Below are customer reviews and the hashtags that describe them:\n{demonstrations_text}\nYou may use the following tags for classification: {tag_list}\nNow classify the following review. Only respond with the list of predicted hashtags.\nReview: {review}\nHashtags:"
- Changes:  
  - Provided the list of valid hashtags in the prompt to minimize hallucinations or invalid outputs.  
  - Added instructions for the model to return only the list of hashtags without extra reasoning or explanation.
- Finding: Surprisingly, this approach reduced performance, possibly because the explicit list made the model overfit or focus too narrowly.

Prompt 3: "Below are customer reviews and the hashtags that describe them:\n{demonstrations_text}\nClassify the following review by selecting only applicable hashtags from the options below. After selecting the options, respond with their corresponding hashtags separated by commas.\nReview: {review}\nOptions:\n{enumerated_tag_list}"
- Changes:
  - Introduced a multiple-choice (MCQ) format, inspired by class discussion, to improve classification accuracy.  
  - Converted the hashtag list into an enumerated format for clarity.  
  - Added explicit instructions for the model to return hashtag strings instead of option numbers.
- Finding: This significantly improved performance, prompting a shift in focus to demonstration selection and order.

Prompt 4: "Below are customer reviews and the hashtags that describe them:\n{demonstrations_text}\nSelect the hashtags from the options below most applicable to the review. Return only the selected hashtags separated by commas.\nReview: {review}\nOptions:\n{bulletpoint_tag_list}"`
- Changes:
  - Simplified wording further to eliminate unnecessary or conflicting information.  
  - Replaced the enumerated list with a bullet-point format to reduce the risk of the model returning option numbers instead of hashtags.
- Finding: This refinement maintained the improved performance of Prompt 3 while further reducing response formatting errors.

- Conclusion -
After experimenting with these variations, I achieved the best results using a multiple-choice format (Prompt 3 and Prompt 4), combined with clear and concise instructions. The focus then shifted to optimizing the selection and order of demonstration examples to further enhance accuracy.

# Justifying demonstration selection and order
Q: Did demonstration selection and order affect your accuracy? What was the best-performing order in your study?

Demonstration selection and order had minimal impact on accuracy overall, but I did observe slight differences based on the ordering strategy used. Here’s a breakdown of the methods I tried and their results:
1. No Ordering (Random Sampling)  
   - Examples were used in the order they were randomly sampled.  
   - Finding: Served as the baseline for comparison. Performance was consistent but unoptimized.

2. Cluster by Similarity  
   - Grouped examples with overlapping or related hashtags together. For instance, examples discussing similar topics (e.g., product quality, performance, or value) were presented consecutively.  
   - Rationale: Clustering similar examples could help the model learn co-occurring patterns and reduce the “cognitive jump” when switching between unrelated examples.  
   - Finding: No significant difference in performance compared to no ordering.

3. Progressive Complexity  
   - Ordered examples from simplest (single hashtag) to most complex (multiple hashtags). Examples were sorted based on the number of hashtags.  
   - Rationale: Starting with simple examples might help the model build a foundational understanding before moving to more complex cases, analogous to how humans learn progressively.  
   - Finding: Again, no significant difference in performance compared to no ordering.

4. Rare Hashtags Last  
   - Prioritized examples with common hashtags at the beginning and moved examples with rare hashtags to the end. This leveraged the model’s recency effect to emphasize rarer hashtags closer to the query.  
   - Rationale: Common hashtags provided a strong foundation early in the prompt, while rare hashtags were emphasized later to improve their prediction accuracy.  
   - Finding: Performance was similar to no ordering.

5. Relevant Hashtags Last  
   - Ordered examples so those with hashtags most relevant to the query were placed closer to the end of the context. Relevance was determined by measuring overlap between example hashtags and query hashtags.  
   - Rationale: The recency effect often causes language models to weigh recent examples more heavily. Placing relevant examples at the end could improve the model’s focus on the specific context of the query.  
   - Finding: This strategy produced the highest F1 score, though the improvement was marginal (~0.02). It also performed best on average across multiple runs, likely due to the model’s sensitivity to the latter examples in the list.

While most ordering strategies had little impact on performance, the last approach achieved the best results, with a slight improvement in F1 score and more consistent performance across runs. This aligns with the observation that language models are influenced by the recency effect when processing demonstrations.

Q: Which number of demonstrations would you choose? Did demonstration selection and order affect your answer?
I would choose 16 demonstrations. After testing various orders with different numbers of demonstrations, 16 consistently yielded the best performance, with no noticeable improvement beyond this point.
The data below supports this (albeit each k value only having one run on the 300 sized test data set), given the .745 f1 score at k=16 compared to the .732 at k=24 and .748 at k=32.

Nonetheless, for this part of the evaluation, I only tested each order and demonstration pair twice, using 20 test cases per run, so my findings may not be entirely accurate.

# The precision, recall and f1-score for the dataset-test evaluation and k demonstrations where k=8, 16, 24, 32
 k |  pre  |  rec  |  f1
 8 | 0.638 | 0.918 | 0.727
16 | 0.682 | 0.886 | 0.745
24 | 0.679 | 0.856 | 0.732
32 | 0.693 | 0.873 | 0.748