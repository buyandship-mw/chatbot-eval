{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c42a95",
   "metadata": {},
   "source": [
    "# Assignment 2 - In-Context Learning\n",
    "\n",
    "In this assignment, students experiment with in-context learning by selecting and ordering demonstrations to train a large language model at inference time to classify text. In this task, an online store is interested in classifying whether a review describes one or more general topics of interest. The topics are specific to a class of product, in this case vacuum cleaners. Other topics would be relevant to other products.\n",
    "\n",
    "The dataset has been divided into a development, training and test sets. Students should practice setting up their experiments and writing their prompts using only the development set. Demonstrations for in-context leanring can be drawn from the training set. Final evaluation prior to submission should use the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f7c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from openai import OpenAI\n",
    "\n",
    "# Read API key from config file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "api_key = config['openai']['api_key']\n",
    "\n",
    "# Create a new OpenAI client\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae8965",
   "metadata": {},
   "source": [
    "## Load Reviews with Hashtags\n",
    "\n",
    "The dataset is partitioned into development, training and testing sets. While writing the code to setup your experiments and write your prompts, only use the development set. The training set should be used to sample demonstrations. Only when your code is completed and you are ready to turn in your assignment should you run your experiment on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa8408ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Sizes: Dev 100, Train 100, Test 300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Used the product and was very happy with it until about a month ago. Motor sounded like it was working harder; thought maybe I was imagining things. Look all through hoses and brush roller assembly for any blockages. Today it was not getting good suction; then motor suddenly cut back on output. Barely runs; does not run in upright position. No suction. Bought this as an \"inexpensive\" replacement to Dyson that died after 5 years. You get what you pay for evidently. Wondering if manufacturer warranty in effect, though I failed to send in the warranty card.',\n",
       " 'expected': ['#PerformanceAndFunctionality',\n",
       "  '#ValueForMoneyAndInvestment',\n",
       "  '#CustomerExperienceAndExpectations'],\n",
       " 'sentiment': ['N', 'N', 'N']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_dev = json.load(open('dataset-dev.json', 'r'))\n",
    "data_train = json.load(open('dataset-train.json', 'r'))\n",
    "data_test = json.load(open('dataset-test.json', 'r'))\n",
    "\n",
    "print('\\nDataset Sizes: Dev %i, Train %i, Test %i\\n' % (len(data_dev), len(data_train), len(data_test)))\n",
    "\n",
    "data_dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c01ddb",
   "metadata": {},
   "source": [
    "## Define the Hashtag List for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07987e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    '#DesignAndUsabilityIssues',\n",
    "    '#PerformanceAndFunctionality',\n",
    "    '#BatteryAndPowerIssues',\n",
    "    '#DurabilityAndMaterialConcerns',\n",
    "    '#MaintenanceAndCleaning',\n",
    "    '#CustomerExperienceAndExpectations',\n",
    "    '#ValueForMoneyAndInvestment',\n",
    "    '#AssemblyAndSetup'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8de86",
   "metadata": {},
   "source": [
    "## Review the Hashtag Distribution\n",
    "\n",
    "In general, it is good practice when classifying items to know the distribution of target categories. Categories that are underrepresented, especially in the training data, would lead to underperformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3540d8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hashtag distribution in training set (sorted by count):\n",
      "#PerformanceAndFunctionality: 58\n",
      "#DurabilityAndMaterialConcerns: 37\n",
      "#CustomerExperienceAndExpectations: 33\n",
      "#ValueForMoneyAndInvestment: 31\n",
      "#DesignAndUsabilityIssues: 18\n",
      "#MaintenanceAndCleaning: 12\n",
      "#AssemblyAndSetup: 7\n",
      "#BatteryAndPowerIssues: 4\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def review_hashtag_distribution(data):\n",
    "    hashtag_counter = Counter()\n",
    "    for item in data:\n",
    "        hashtag_counter.update(item['expected'])\n",
    "    \n",
    "    return hashtag_counter\n",
    "\n",
    "# Review hashtag distribution for the training set\n",
    "distribution = review_hashtag_distribution(data_train)\n",
    "\n",
    "# Sort by count in descending order\n",
    "sorted_distribution = distribution.most_common()\n",
    "\n",
    "print(\"Hashtag distribution in training set (sorted by count):\")\n",
    "for tag, count in sorted_distribution:\n",
    "    print(f\"{tag}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c52bf",
   "metadata": {},
   "source": [
    "## Define the Prompt and Experiment\n",
    "\n",
    "The experiment generally has the following steps: (1) sample the training data to identify k demonstrations for 0 =< k < training set size; (2) construct linearize the demonstrations into text; (3) iterate over the test data and insert the test review and text linearization of the demonstrations into the prompt template; (4) send the prompt to the model and receive the response; (5) validate the response, if the response passes then store the response for later, else if the response fails validation, then save the response to a list of errors. It is generally good to save responses and errors with an index that can be linked back to the test data.\n",
    "\n",
    "After running the experiment, the evaluation metrics should be computed from the answers and the errors should be inspected. Adjustments to the prompt and/or experiment can be made to reduce the errors, e.g., by post-processing the responses prior to validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04107f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "Experiment completed. 300 results saved to 'results.json'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "def sample_demonstrations(dataset, k):\n",
    "    \"\"\"\n",
    "    Samples k random demonstrations from the dataset.\n",
    "    \"\"\"\n",
    "    return random.sample(dataset, k)\n",
    "\n",
    "## Where the prompt engineering begins\n",
    "def linearize_demonstrations(demonstrations):\n",
    "    \"\"\"\n",
    "    Converts a list of demonstration examples into a formatted text string.\n",
    "    \"\"\"\n",
    "    prompt_text = \"\"\n",
    "    for demo in demonstrations:\n",
    "        prompt_text += f\"Review: {demo['text']}\\n\"\n",
    "        prompt_text += f\"Hashtags: {', '.join(demo['expected'])}\\n\\n\"\n",
    "    return prompt_text\n",
    "\n",
    "# ordering strategies start\n",
    "def cluster_by_similarity(demonstrations):\n",
    "    \"\"\"\n",
    "    Orders examples by clustering based on overlapping hashtags.\n",
    "    \"\"\"\n",
    "    # Create a map of hashtags to examples\n",
    "    hashtag_map = {}\n",
    "    for demo in demonstrations:\n",
    "        for hashtag in demo[\"expected\"]:\n",
    "            if hashtag not in hashtag_map:\n",
    "                hashtag_map[hashtag] = []\n",
    "            hashtag_map[hashtag].append(demo)\n",
    "\n",
    "    # Cluster examples by hashtag\n",
    "    clustered_examples = []\n",
    "    seen = set()\n",
    "    for hashtag, examples in hashtag_map.items():\n",
    "        for example in examples:\n",
    "            if id(example) not in seen:  # Prevent duplicates\n",
    "                clustered_examples.append(example)\n",
    "                seen.add(id(example))\n",
    "\n",
    "    return linearize_demonstrations(clustered_examples)\n",
    "\n",
    "def progressive_complexity(demonstrations):\n",
    "    \"\"\"\n",
    "    Orders examples by increasing complexity (fewer hashtags to more hashtags).\n",
    "    \"\"\"\n",
    "    sorted_examples = sorted(demonstrations, key=lambda demo: len(demo[\"expected\"]))\n",
    "    return linearize_demonstrations(sorted_examples)\n",
    "\n",
    "def rare_hashtags_last(demonstrations):\n",
    "    \"\"\"\n",
    "    Orders examples so that those with rare hashtags are placed last in the context.\n",
    "    \"\"\"\n",
    "    # Count the frequency of each hashtag in the dataset\n",
    "    hashtag_counts = {}\n",
    "    for demo in demonstrations:\n",
    "        for hashtag in demo[\"expected\"]:\n",
    "            hashtag_counts[hashtag] = hashtag_counts.get(hashtag, 0) + 1\n",
    "\n",
    "    # Sort examples based on the rarity of their hashtags\n",
    "    sorted_examples = sorted(\n",
    "        demonstrations,\n",
    "        key=lambda demo: min(hashtag_counts[hashtag] for hashtag in demo[\"expected\"]),\n",
    "        reverse=False\n",
    "    )\n",
    "    return linearize_demonstrations(sorted_examples)\n",
    "\n",
    "def relevant_hashtags_last(demonstrations, query_hashtags):\n",
    "    \"\"\"\n",
    "    Orders examples so that those most relevant to the query hashtags are placed last.\n",
    "    \"\"\"\n",
    "    sorted_examples = sorted(\n",
    "        demonstrations,\n",
    "        key=lambda demo: len(set(demo[\"expected\"]) & set(query_hashtags)),\n",
    "    )\n",
    "    return linearize_demonstrations(sorted_examples)\n",
    "# ordering strategies end\n",
    "\n",
    "def construct_prompt(demonstrations_text, review, tag_list):\n",
    "    \"\"\"\n",
    "    Constructs the full prompt for the model.\n",
    "    \n",
    "    Parameters:\n",
    "        demonstrations_text (str): The formatted text of the demonstrations.\n",
    "        review (str): The text of the review to classify.\n",
    "        tag_list (list): The full list of valid hashtags for prediction.\n",
    "    \n",
    "    Returns:\n",
    "        str: The constructed prompt as a string.\n",
    "    \"\"\"\n",
    "    bulletpoint_tag_list = \"\\n\".join([f\"- {tag}\" for tag in tag_list])\n",
    "    prompt = f\"Below are customer reviews and the hashtags that describe them:\\n{demonstrations_text}\"\n",
    "    prompt += \"Select the hashtags from the options below most applicable to the review. Return only the selected hashtags separated by commas.\\n\"\n",
    "    prompt += f\"Review: {review}\\n\"\n",
    "    prompt += f\"Options:\\n{bulletpoint_tag_list}\"\n",
    "    return prompt\n",
    "## Where the prompt engineering ends\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    \"\"\"\n",
    "    Sends the prompt to the model and returns the response.\n",
    "    \"\"\"\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def extract_valid_hashtags(response, tag_list):\n",
    "    \"\"\"\n",
    "    Extracts valid hashtags from the model's response.\n",
    "    \n",
    "    Parameters:\n",
    "        response (str): The raw response string from the model.\n",
    "        tag_list (list): The list of valid hashtags.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of valid hashtags extracted from the response.\n",
    "    \"\"\"\n",
    "    # Normalize the response by removing extra spaces and splitting into lines\n",
    "    response = response.strip()\n",
    "    \n",
    "    # Use a regular expression to extract all words starting with '#'\n",
    "    extracted_tags = re.findall(r\"#\\w+\", response)\n",
    "    \n",
    "    # Filter the extracted tags to include only those in the valid tag list\n",
    "    valid_tags = [tag for tag in extracted_tags if tag in tag_list]\n",
    "    \n",
    "    return valid_tags\n",
    "\n",
    "def save_to_json(filename, data):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "# Parameters\n",
    "k = 8  # Number of demonstrations to include\n",
    "tag_list = tags  # Use the global tag list\n",
    "\n",
    "# Run the experiment\n",
    "results = []\n",
    "errors = []\n",
    "test_cases = data_test # the reviews to classify\n",
    "\n",
    "# Sample and linearize demonstrations\n",
    "demonstrations = sample_demonstrations(data_train, k)\n",
    "\n",
    "# Order and linearalize demonstrations\n",
    "# demonstrations_text = your_strategy_here(demonstrations)\n",
    "    \n",
    "for idx, test_data in enumerate(test_cases):\n",
    "    # This strategy needs to be computed here since it relies on the review to be predicted\n",
    "    demonstrations_text = relevant_hashtags_last(demonstrations, test_data['expected'])\n",
    "\n",
    "    # Construct the full prompt and get response from the model\n",
    "    prompt = construct_prompt(demonstrations_text, test_data['text'], tag_list)\n",
    "    try:\n",
    "        response = prompt_model(prompt)\n",
    "        print(idx)\n",
    "        \n",
    "        # Remove invalid tags from the response\n",
    "        predicted_categories = extract_valid_hashtags(response, tag_list)\n",
    "        results.append({\n",
    "            \"review\": test_data[\"text\"],\n",
    "            \"true_labels\": test_data[\"expected\"],\n",
    "            \"predicted\": predicted_categories,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        # Catch and log any other errors\n",
    "        errors.append({\n",
    "            \"review\": test_data[\"text\"],\n",
    "            \"true_labels\": test_data[\"expected\"],\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "# Save the results and errors to files\n",
    "save_to_json(\"results.json\", results)\n",
    "save_to_json(\"errors.json\", errors)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Experiment completed. {len(results)} results saved to 'results.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4c7f6",
   "metadata": {},
   "source": [
    "## Evaluate the Experimental Results\n",
    "\n",
    "The evaluation metrics include precision, recall and F1 score. For the total number of true positives (tp), false positives (fp) and false negatives (fn), these calculations should be used to report results:\n",
    "* Precision = tp / (tp + fp)\n",
    "* Recall = tp / (tp + fn)\n",
    "* F1 = 2tp / (2tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d522e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Precision: 0.638\n",
      "Recall: 0.918\n",
      "F1 Score: 0.727\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_from_json(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def evaluate_results(results):\n",
    "    \"\"\"\n",
    "    Evaluates the results of the experiment by calculating overall precision, recall, and F1 score.\n",
    "    Metrics are calculated per instance and then averaged across all results.\n",
    "    \n",
    "    Parameters:\n",
    "        results (list): The list of results. Each result object should contain a list of true and a list of predicted labels.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing overall precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1 = 0\n",
    "    \n",
    "    for result in results:\n",
    "        true_labels = result[\"true_labels\"]\n",
    "        predicted_labels = result[\"predicted\"]\n",
    "        \n",
    "        # Count true positives, false positives, and false negatives\n",
    "        tp = 0  # True positives\n",
    "        fp = 0  # False positives\n",
    "        fn = 0  # False negatives\n",
    "\n",
    "        # Count occurrences in true and predicted lists\n",
    "        true_counts = {}\n",
    "        predicted_counts = {}\n",
    "\n",
    "        for label in true_labels:\n",
    "            true_counts[label] = true_counts.get(label, 0) + 1\n",
    "\n",
    "        for label in predicted_labels:\n",
    "            predicted_counts[label] = predicted_counts.get(label, 0) + 1\n",
    "\n",
    "        # Calculate true positives\n",
    "        for label in predicted_counts:\n",
    "            if label in true_counts:\n",
    "                tp += min(predicted_counts[label], true_counts[label])\n",
    "\n",
    "        # Calculate false positives\n",
    "        for label in predicted_counts:\n",
    "            if label not in true_counts:\n",
    "                fp += predicted_counts[label]\n",
    "            else:\n",
    "                fp += max(0, predicted_counts[label] - true_counts[label])\n",
    "\n",
    "        # Calculate false negatives\n",
    "        for label in true_counts:\n",
    "            if label not in predicted_counts:\n",
    "                fn += true_counts[label]\n",
    "            else:\n",
    "                fn += max(0, true_counts[label] - predicted_counts[label])\n",
    "\n",
    "        # Precision, recall, and F1 for this result\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "\n",
    "        # Accumulate metrics\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "    \n",
    "    # Calculate averages\n",
    "    n = len(results)\n",
    "    return {\n",
    "        \"precision\": total_precision / n,\n",
    "        \"recall\": total_recall / n,\n",
    "        \"f1\": total_f1 / n,\n",
    "    }\n",
    "\n",
    "# Evaluate the results\n",
    "results = read_from_json(\"results.json\")\n",
    "metrics = evaluate_results(results)\n",
    "\n",
    "# Print the overall metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"F1 Score: {metrics['f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4d0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5 demos, 20 test cases, best of 2 runs\n",
    "# no ordering: .66, .89, .74\n",
    "# cluster by similarity: .66, .94, .76\n",
    "# progressive complexity: .68, .94, .78\n",
    "# rare hashtags last: .67, .90, .75\n",
    "# relevant hashtags last: .74, .88, .78\n",
    "\n",
    "## 10 demos, 20 test cases, best of 2 runs\n",
    "# no ordering: .69, .93, .78\n",
    "# cluster by similarity: .72, .89, .78\n",
    "# progressive complexity: .70, .90, .77\n",
    "# rare hashtags last: .71, .92, .78\n",
    "# relevant hashtags last: .71, .93, .80\n",
    "\n",
    "## relevant hashtags last, 20 test cases, 1 run f1-score\n",
    "# 8 demos: .787\n",
    "# 9 demos: .709\n",
    "# 10 demos: .721"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
